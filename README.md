# Awesome EfficientAI for MLLM

A curated list of awesome resources, research papers, and tools for optimizing and efficiently utilizing Multi-Large Language Models (MLLMs). This repository is designed to serve as a one-stop resource hub for developers, researchers, and enthusiasts aiming to build efficient AI solutions.

---

## ðŸ“– Table of Contents

- [Introduction](#introduction)
- [Categories](#categories)
  - [Optimization Techniques](#optimization-techniques)
  - [Model Compression](#model-compression)
  - [Deployment and Acceleration](#deployment-and-acceleration)
  - [Multimodal Integration](#multimodal-integration)
- [License](#license)

---

## Introduction

The rise of large language models (LLMs) like GPT, BERT, and LLaMA has brought new challenges in efficiently deploying and utilizing these models.
**Awesome EfficientAI for MLLM** collects recent research papers, tools, and resources that focus on optimizing and accelerating multi-modal large language models (MLLMs). 
The repository aims to provide a comprehensive overview of the latest advancements in model compression, deployment, and optimization techniques for MLLMs.

## Categories

### Optimization Techniques

#### Papers

- **Pruning**: Methods for removing unnecessary weights to make models smaller and faster.  
  - FastV: An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models [Paper](https://arxiv.org/pdf/2403.06764)  [Code](https://github.com/pkunlp-icler/FastV)
  - SparseVLM: Visual Token Sparsification for Efficient Vision-Language Model Inference [Paper](https://arxiv.org/abs/2410.04417) [Code](https://github.com/Gumpest/SparseVLMs)
  - VisionZip: Longer is Better but Not Necessary in Vision Language Models [Paper](https://arxiv.org/abs/2412.04467) [Code](https://github.com/dvlabresearch/VisionZip)
  - FiCoCo: Rethinking Token Reduction in MLLMs: Towards a Unified Paradigm for Training-Free Acceleration [Paper](https://arxiv.org/pdf/2411.17686) [Code](https://ficoco-accelerate.github.io/)
  - AIM: Adaptive Inference of Multi-Modal LLMs via Token Merging and Pruning [Paper](https://arxiv.org/pdf/2412.03248) [Code](https://github.com/LaVi-Lab/AIM)


- **New Architectures**: Novel model architectures designed for efficiency.  
  - Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference [Paper](https://arxiv.org/abs/2403.14520) [Code](https://github.com/h-zhao1997/cobra)

---

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=924973292/Awesome-EfficientAI-for-MLLM&type=Date)](https://star-history.com/#924973292/Awesome-EfficientAI-for-MLLM&Date)

## Contact

Feel free to reach out if you have any questions, suggestions, or collaboration proposals:

- Email: [924973292@mail.dlut.edu.cn](mailto:924973292@mail.dlut.edu.cn)
- Web: [924973292.github.io](https://924973292.github.io//)

## License

This project is licensed under the [MIT License](LICENSE).

---

Thank you for supporting **Awesome EfficientAI for MLLM**! Letâ€™s build an efficient AI ecosystem ðŸš€
